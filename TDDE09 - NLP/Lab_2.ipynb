{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Due date:** 2017-02-03\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Language Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students:** Johan Lindström (johli160), Jonathan Sjölund (jonsj507)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will experiment with $n$-gram models. You will test various parameters that influence these models&rsquo; quality and train to estimate models with additive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code import the Python modules needed for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nlp2\n",
    "import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this lab consists of Arthur Conan Doyle&rsquo;s novels about Sherlock Holmes: *The Adventures of Sherlock Holmes*, *The Memoirs of Sherlock Holmes*, *The Return of Sherlock Holmes*, *His Last Bow* and *The Case-Book of Sherlock Holmes*. The next piece of code loads the first three of these as training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = nlp2.read_data(\"/home/TDDE09/labs/nlp2/data/advs.txt\",\n",
    "                               \"/home/TDDE09/labs/nlp2/data/mems.txt\",\n",
    "                               \"/home/TDDE09/labs/nlp2/data/retn.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is represented as a list of sentences, where one sentence is represented as a list of tokens (strings). The next line prints the 101th sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'Let', 'us', 'glance', 'at', 'our', 'Continental', 'Gazetteer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(training_data[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between a model’s quality and its order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this lab you will examine the relation between an $n$-gram model’s quality and its **order**, i.e. the value of&nbsp;$n$. You will do both a qualitative and quantitative evaluation with the help of the entropy measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line trains a bigram-model of the class `ngrams.Model` on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nlp2.train(ngrams.Model, 10, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model you are able to generate random sentences. Every time you run the following code cell a new sentence is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \" It is a privilege to be associated with you in the handling of a case , \" said the inspector , warmly .\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(model.generate()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the sentences. Do they sound natural?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 1</div>\n",
    "<div class=\"panel-body\">\n",
    "Train a unigram-, bigram-, trigram-, and quadrigram-model, and generate random sentences with each. How does the quality of the sentences change with the model’s order? Explain your observations using your understanding from how an $n$-gram model works. Use some generated sentences in order to illustrate your discussion. How would the sentences look like for higher values of $n$, such as $n=10$?\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Insert your code here\n",
    "model1 = nlp2.train(ngrams.Model, 1, training_data)\n",
    "model2 = nlp2.train(ngrams.Model, 2, training_data)\n",
    "model3 = nlp2.train(ngrams.Model, 3, training_data)\n",
    "model4 = nlp2.train(ngrams.Model, 4, training_data)\n",
    "model10 = nlp2.train(ngrams.Model, 10, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It increases much from unigram to quadrigram, and from quadrigram to 10-gram the change is not as big but it is clear that 10-gram is better. Here are some sentences for unigram, bigram, trigram... etc!\n",
    "\n",
    "Unigram: problem them invisible better She all did ; must between a , gave as kind-hearted to have\n",
    "\n",
    "Bigram: He had a day , ' \" Pray tell us as usual size , and to the thought , strong , that I am a very old man , Watson , as silent course the form of the professional commission of such a boot-lace .\n",
    "\n",
    "Trigram: \" \" You'll never persuade me to hear the tones that he had described .\n",
    "\n",
    "Quadrigram: You made some small job in my lady's room -- you and your confederate Cusack -- and you managed that he should retain the young Swiss messenger with him as guide and companion while I returned to Baker Street , and remember the advice which I have for this fellow .\n",
    "\n",
    "10-gram: The tip had been cut off , not bitten off , but the cut was not a clean one , so I deduced a blunt pen-knife .\n",
    "\n",
    "If we look at the unigram sentence it is completely incoherent, for the bigram we can see at least a sentence that can be read but it is still incoherent. The trigram sentence is understandable in some sense, it is drastical change from unigram and clearly better than bigram. The quadrigram sentence is longer so we can see that the coherency is still a bit off, a couple of words are weird. The difference between quadrigram and trigram is not as big as the changes between unigram and bigram as well as the difference between bigram and trigram. For the 10-gram we can clearly see that it is the best sentence becuase we look at the 10 previous words which some sentences are, thus producing the best result. The reason for these changeses is becuase the number of options for a given n-gram decreases when n increases, thus the sentece created will likely be a very coherent one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do a quantitative evaluation of a model we can compute its **entropy** on held-out data. We will use the first part of the novel *The Adventures of Sherlock Holmes* for this. It is loaded by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = nlp2.read_data(\"/home/TDDE09/labs/nlp2/data/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece of code trains a bigram-model and computes its entropy on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.426862596420277"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nlp2.train(ngrams.Model, 2, training_data)\n",
    "nlp2.evaluate(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 2</div>\n",
    "<div class=\"panel-body\">\n",
    "Compute the entropy for the four models you created for the previous problem. How does the model’s entropy change with the model’s order? Explain using your knowledge of the entropy measure.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.337551182974018\n",
      "3.426862596420277\n",
      "1.4289533769461726\n",
      "0.5436027106964166\n",
      "0.3321634449144811\n"
     ]
    }
   ],
   "source": [
    "# TODO: Insert your code here\n",
    "ent1=nlp2.evaluate(model1, test_data)\n",
    "ent2=nlp2.evaluate(model2, test_data)\n",
    "ent3=nlp2.evaluate(model3, test_data)\n",
    "ent4=nlp2.evaluate(model4, test_data)\n",
    "ent10=nlp2.evaluate(model10, test_data)\n",
    "print(ent1)\n",
    "print(ent2)\n",
    "print(ent3)\n",
    "print(ent4)\n",
    "print(ent10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is a measure of efficiency or disorder (depending on context) for a system. In this case we can see it more as a measure of how \"suprised\" we are to see the word. This means that more entropy means that we are more \"suprised\" over the result which often means that the word predicted is probably not the right one. If the entropy is low we are more sure that the word predicted is the right one which translates perfectly for when someone sends messages on their phone without looking at the keyboard and produces the information desired in the message. As we can see, the higher model we have the lower the entropy gets since we look at more previous words the next suggested word will suprises us less because it will be coherent for the sentence we are writing. From unigram to bigram the entropy decreases alot, same for bigram to trigram and after that the decreases is not as big which we can see from quadrigram to n-gram. This means that for higher n-grams the \"supprise\" change wont be so different which means that at some point it is not worth increasing the n-gram since it will not impact the result that much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between a model’s quality and the estimation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of this lab you will implement and evaluate various estimation methods. In order to do that you will need to know how the lab system is built up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The content of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call the `train()` function (like you did above), the system creates an $n$-gram model of the given class (so far: `ngrams.Model`) and with the given order (the value of $n$) and trains the model on the given data set. For the second part of this lab you will use your own model class. We start with defining the class in such a way that it simply calls the corresponding methods of the superclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def order(self):\n",
    "        \"\"\"Return the order of this model (an integer).\"\"\"\n",
    "        return super().order()\n",
    "    \n",
    "    def vocabulary(self):\n",
    "        \"\"\"Return this model's vocabulary (a set).\"\"\"\n",
    "        return super().vocabulary()\n",
    "    \n",
    "    def freq(self, ctxt, word):\n",
    "        \"\"\"Return the number of occurrences of `word` (a string) after `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().freq(ctxt, word)\n",
    "    \n",
    "    def total(self, ctxt):\n",
    "        \"\"\"Return the total number of ngrams that start with `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().total(ctxt)\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().prob(ctxt, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece of code trains a bigram-model of the class `Model` and prints the model’s order (an integer) and the size of its vocabulary (a set of strings, represented by Python’s `set` type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order of the model: 2\n",
      "number of words in the model's vocabulary: 15339\n"
     ]
    }
   ],
   "source": [
    "model = nlp2.train(Model, 2, training_data)\n",
    "print(\"order of the model:\", model.order())\n",
    "print(\"number of words in the model's vocabulary:\", len(model.vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up an n-gram’s absolute frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trained model consists primarily of a table with absolute frequencies for all $n$-grams that appear in the text it was trained on. In order to look up an $n$-gram’s absolute frequency one can use the method `freq()`. An $n$-gram is divided into two parts: an $(n-1)$-gram called **context** (`ctxt`) and a final unigram (`word`). In Python the context is represented as a tuple of strings and the unigram as a normal string.\n",
    "\n",
    "If you want to train a trigram model and then know the absolute frequency for the trigram *Mr. Sherlock Holmes* you can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nlp2.train(Model, 3, training_data)\n",
    "model.freq((\"Mr.\", \"Sherlock\"), \"Holmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a bigram model and looking up the absolute frequency for the bigram *Baker Street* you can write the following. Note that the context of a bigram model is a 1-tuple of strings, which has a special notation in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nlp2.train(Model, 2, training_data)\n",
    "model.freq((\"Baker\",), \"Street\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up the absolute frequency of an n-gram with a given context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `total()` returns the absolute frequency of $n$-grams with the given context. Here is an example for a trigram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nlp2.train(Model, 3, training_data)\n",
    "model.total((\"Mr.\", \"Sherlock\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 3</div>\n",
    "<div class=\"panel-body\">\n",
    "Train a bigram model and use it to calculate the following values, using the methods shown above.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nlp2.train(Model, 2, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.** the absolute frequency for the bigram *Sherlock Holmes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Insert your code here\n",
    "model.freq((\"Sherlock\",),\"Holmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.** the absolute frequency of bigrams with the context *Sherlock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Insert your code here\n",
    "model.total((\"Sherlock\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.** the absolute frequency for the unigram *Sherlock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Insert your code here\n",
    "model.total((\"Sherlock\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.** the number of words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15339"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Insert your code here\n",
    "len(model.vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.** a list with all the words following the context *Sherlock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Holmes's\", 'looked', 'everywhere', 'Holmes', '?', ',', '!', 'has', '.']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Insert your code here\n",
    "temp = model.vocabulary()\n",
    "temp = list(temp)\n",
    "listOfWords = []\n",
    "templist = [\"\"]\n",
    "for word in temp:\n",
    "    if(model.freq((\"Sherlock\",),word) > 0):\n",
    "        templist[0] = word\n",
    "        listOfWords = listOfWords+templist\n",
    "print(listOfWords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For the last exercise you will need to write a bit more than a simple function call.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate probabilities with the Maximum Likelihood method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `prob()` returns the estimated conditional probability $P(w|c)$ for a word $w$ given a context $c$. The following code snippet trains a trigram model and estimates the pobability for *Holmes* given the context *Mr. Sherlock*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nlp2.train(Model, 3, training_data)\n",
    "model.prob((\"Mr.\", \"Sherlock\"), \"Holmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(What does the returned value imply?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 4</div>\n",
    "<div class=\"panel-body\">\n",
    "Do your own implementation of the method `prob()`. The method should estimate probabilities using the Maximum Likelihood method. Test your implementation by redoing Exercise&nbsp;2 with the new class; you should get the same result as before. Use the code you wrote in Exercise&nbsp;3 in order to solve the exercise.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.337551182974018\n",
      "3.426862596420277\n",
      "1.4289533769461726\n",
      "0.5436027106964166\n"
     ]
    }
   ],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        # TODO: Replace the next line with your own code \n",
    "        freqOfngram = self.freq(ctxt,word)\n",
    "        freqOfTuplegram = self.total(ctxt)\n",
    "            \n",
    "        return freqOfngram/freqOfTuplegram\n",
    "\n",
    "# TODO: Insert your testing code here\n",
    "model1 = nlp2.train(Model, 1, training_data)\n",
    "model2 = nlp2.train(Model, 2, training_data)\n",
    "model3 = nlp2.train(Model, 3, training_data)\n",
    "model4 = nlp2.train(Model, 4, training_data)\n",
    "\n",
    "ent1=nlp2.evaluate(model1, test_data)\n",
    "ent2=nlp2.evaluate(model2, test_data)\n",
    "ent3=nlp2.evaluate(model3, test_data)\n",
    "ent4=nlp2.evaluate(model4, test_data)\n",
    "\n",
    "print(ent1)\n",
    "print(ent2)\n",
    "print(ent3)\n",
    "print(ent4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve this exercise you will need to turn the formula for Maximum Likelihood estimation into code. We illustrate the formula for a bigram model. If we write $f(w_1w_2)$ for the number of occurrences of the bigram  $w_1w_2$ and $f(w_1)$ for the number of occurrences of the unigram $w_1$, then the probability for observing $w_2$ given $w_1$ is\n",
    "$$\n",
    "P(w_2|w_1) = \\frac{f(w_1w_2)}{f(w_1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with Maximum Likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `yoda.txt` contains the same text as `test.txt`, but in the jumbled [Yoda-language]( http://itre.cis.upenn.edu/~myl/languagelog/archives/002173.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yoda_data = nlp2.read_data(\"/home/TDDE09/labs/nlp2/data/yoda.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 5</div>\n",
    "<div class=\"panel-body\">\n",
    "Redo the evaluation of the four previous models with `yoda.txt` as test data. Something unexpected happens for models with $n>1$. Why? Explain the problem with your knowledge of Maximum Likelihood estimation.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.2441064060866385\n"
     ]
    }
   ],
   "source": [
    "# TODO: Insert your code here\n",
    "ent1=nlp2.evaluate(model1, yoda_data)\n",
    "#ent2=nlp2.evaluate(model2, yoda_data)\n",
    "#ent3=nlp2.evaluate(model3, yoda_data)\n",
    "#ent4=nlp2.evaluate(model4, yoda_data)\n",
    "\n",
    "print(ent1)\n",
    "#print(ent2)\n",
    "#print(ent3)\n",
    "#print(ent4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For n = 1 the model will be a unigram model, which means that as long as the word exist we will always get a probability which can be a problem for higher n-gram models. For higher model it seems that atleast one word combination will have the probability of 0 % which is a problem in the context of how the evaluation for maximum likelihood estimation is done. For unigram it works and we get an entropy, almost similar to the one we got previously, which means that in order to avoid underflow the evalutation will take the log of each probability which is the problem here. If just one probability is zero then it will ruin the whole evaluation which is the case here for n > 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate probabilities with additive smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next problem you are going to do Maximum Likelihood estimation, but with additive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 6</div>\n",
    "<div class=\"panel-body\">\n",
    "<p>\n",
    "Write a new implementation of the method `prob()`, such that it estimates probabilities with additive smoothing.</p>\n",
    "<p>\n",
    "Evaluate the system with new new class using the entropy measure from Problem&nbsp;2. Choose the following values for the smoothing constant $k$:&nbsp;0,00,&nbsp;1,00, 0,10, 0,01. For $k=0$ you should get the same result as in Problem&nbsp;5.\n",
    "</p>\n",
    "<p>\n",
    "How and why does the smoothing constant influence the model’s entropy? Connect to the distribution of the probability mass between observed and imaginary occurrences.\n",
    "</p>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.3366010522328216\n",
      "4.813092274317304\n",
      "4.767917212034177\n",
      "4.854491062575155\n"
     ]
    }
   ],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        # TODO: Replace the next line with your own code\n",
    "        freqOfngram = self.freq(ctxt,word)\n",
    "        freqOfTuplegram = self.total(ctxt)\n",
    "        k = 0.01\n",
    "            \n",
    "        return (freqOfngram+k)/(freqOfTuplegram+k*len(self.vocabulary()))\n",
    "\n",
    "# TODO: Insert your testing code here\n",
    "model1 = nlp2.train(Model, 1, training_data)\n",
    "model2 = nlp2.train(Model, 2, training_data)\n",
    "model3 = nlp2.train(Model, 3, training_data)\n",
    "model4 = nlp2.train(Model, 4, training_data)\n",
    "\n",
    "ent1=nlp2.evaluate(model1, test_data)\n",
    "ent2=nlp2.evaluate(model2, test_data)\n",
    "ent3=nlp2.evaluate(model3, test_data)\n",
    "ent4=nlp2.evaluate(model4, test_data)\n",
    "\n",
    "print(ent1)\n",
    "print(ent2)\n",
    "print(ent3)\n",
    "print(ent4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the table below with your entropy measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><td></td><td>k = 0,00</td><td>k = 1,00</td><td>k = 0,10</td><td>k = 0,01</td></tr>\n",
    "<tr><td>n = 1</td><td>7.337551182974018/td><td>7.273171406469247</td><td>7.328460956945743</td><td>7.3366010522328216</td></tr>\n",
    "<tr><td>n = 2</td><td>3.426862596420277</td><td>7.383667454674711</td><td>5.983399629244003</td><td>4.813092274317304</td></tr>\n",
    "<tr><td>n = 3</td><td>1.4289533769461726</td><td>8.457347218116801</td><td>6.733205552260075</td><td>4.767917212034177</td></tr>\n",
    "<tr><td>n = 4</td><td>0.5436027106964166</td><td>8.657735266369459</td><td>6.955629963762169</td><td>4.854491062575155</td></tr>\n",
    "</table>\n",
    "\n",
    "The smoothing constant influence the model's entropy in the way that the probability for a specific word decreases, thus making it less likely to be choosen. The reason for this is because the smoothing only changes the number of words, by increasing it, the probability itself does not increase in any way thus causing this phenomenon. When we add smoothing we take away a certain percentage of the probability and redistribute it equally to all other words. This effect shows clearly in the table when k = 1 the entropy increases for each n-gram higher than unigram and when k decreases the entropy decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An unseen test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your last exercise is to redo the evaluation on a previously unseen test set, texts from the collection *His Last Bow*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unseen_data = nlp2.read_data(\"/home/TDDE09/labs/nlp2/data/lstb.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 7</div>\n",
    "<div class=\"panel-body\">\n",
    "Redo the evaluation from Problem 6 with the new test data. Explain what happens given the differences between `test.txt` and `lstb.txt`.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.160241085953113\n",
      "6.81990718604588\n",
      "8.421607621810862\n",
      "8.860184828789535\n"
     ]
    }
   ],
   "source": [
    "# TODO: Insert your code here\n",
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        # TODO: Replace the next line with your own code\n",
    "        temp = self.vocabulary()\n",
    "        k = 1.0\n",
    "        wordExist = True\n",
    "        for i in ctxt:\n",
    "            if(i not in temp):\n",
    "                wordExist = False\n",
    "        \n",
    "        if(word in temp and wordExist == True):\n",
    "            freqOfngram = self.freq(ctxt,word)\n",
    "            freqOfTuplegram = self.total(ctxt)  \n",
    "            return (freqOfngram+k)/(freqOfTuplegram+k*len(self.vocabulary()))\n",
    "        else:\n",
    "            return k/(k*len(self.vocabulary()))\n",
    "\n",
    "# TODO: Insert your testing code here\n",
    "model1 = nlp2.train(Model, 1, training_data)\n",
    "model2 = nlp2.train(Model, 2, training_data)\n",
    "model3 = nlp2.train(Model, 3, training_data)\n",
    "model4 = nlp2.train(Model, 4, training_data)\n",
    "\n",
    "ent1=nlp2.evaluate(model1, unseen_data )\n",
    "ent2=nlp2.evaluate(model2, unseen_data )\n",
    "ent3=nlp2.evaluate(model3, unseen_data )\n",
    "ent4=nlp2.evaluate(model4, unseen_data )\n",
    "\n",
    "print(ent1)\n",
    "print(ent2)\n",
    "print(ent3)\n",
    "print(ent4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between test and lstb is that lstb contains words that did not appear in the training data. The entropy for lstb is lower for model 1,2,3 when k = 1. The entropy for lstb is lower for model 1,2 when k = 0.1. The entropy for lstb is lower for the model 1 when k = 0.01. Here we see that when we can get a better evaluating model for unseen data if we use smoothing. It seems that the evaluating model have more trouble with ngrams of higher order if the test data contains unknown words. If we increase the smoothing factor k more we alleviates this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
