{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Due date:** 2017-02-17\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Syntactic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students:** Johan Lindström (johli160), Jonathan Sjölund (jonsj507)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntactic analysis, also called syntactic parsing, is the task of mapping a sentence to a formal representation of its syntactic structure. In this lab you will implement a syntactic parser that produces **dependency trees**. A dependency tree consists of directed arcs between individual words (tokens) of a sentence. To see some examples, have a look at the [Syntax: General Principles](http://universaldependencies.org/u/overview/syntax.html) page of the [Universal Dependencies Project](http://universaldependencies.org/).\n",
    "\n",
    "Your starting point for this lab is a complete reference implementation of a syntactic parser in the Python module `nlp4`. This system (which consists of roughly 200 lines of code) implements a simple pipeline architecture that includes a variant of the part-of-speech tagger that you implemented in lab&nbsp;3 and a transition-based dependency parser based on the arc-standard algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nlp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference system has the following components:\n",
    "\n",
    "1. code to read in dependency trees in the [CoNLL-U Format](http://universaldependencies.org/format.html)\n",
    "2. a class that implements a multi-class perceptron classifier (lab&nbsp;1)\n",
    "3. a class that implements a part-of-speech tagger (lab&nbsp;3)\n",
    "4. a class that implements a transition-based dependency parser (this lab)\n",
    "5. code to train and evaluate the parser on a treebank in the CoNLL-U format\n",
    "\n",
    "These components are explained in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "For this lab you only need to implement the functions that read dependency trees (Problem&nbsp;1) and the class that implements the parser (Problem&nbsp;2). However, to do that you will also need to use the other components.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set that you will be using in this lab is the [English Web Treebank](http://universaldependencies.org/en/overview/introduction.html) from the Universal Dependencies Project. This is a corpus of written English taken from five genres of web media: weblogs, newsgroups, emails, reviews, and Yahoo! answers. Each sentence in the corpus was semi-automatically annotated with (among other things) part-of-speech tags and syntactic dependency trees.\n",
    "\n",
    "In the original files from the Universal Dependencies Project, annotated sentences are stored in the [CoNLL-U Format](http://universaldependencies.org/format.html). This is a simple text-based format where a sentence consists of one line per word, and each line contains 10 tab-separated fields with various pieces of information about the word in question &ndash; including the word form and the part-of-speech tag.\n",
    "\n",
    "The following code cell prints a concrete example, which shows how the data looks like when it comes to Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'From', 'from', 'ADP', 'IN', '_', '3', 'case', '_', '_'], ['2', 'the', 'the', 'DET', 'DT', 'Definite=Def|PronType=Art', '3', 'det', '_', '_'], ['3', 'AP', 'AP', 'PROPN', 'NNP', 'Number=Sing', '4', 'nmod', '_', '_'], ['4', 'comes', 'come', 'VERB', 'VBZ', 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin', '0', 'root', '_', '_'], ['5', 'this', 'this', 'DET', 'DT', 'Number=Sing|PronType=Dem', '6', 'det', '_', '_'], ['6', 'story', 'story', 'NOUN', 'NN', 'Number=Sing', '4', 'nsubj', '_', '_'], ['7', ':', ':', 'PUNCT', ':', '_', '4', 'punct', '_', '_']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/TDDE09/labs/nlp4/en-ud-dev.conllu\") as fp:\n",
    "    print(next(nlp4.conllu(fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each sentence is represented as a list of lists of strings. The inner lists correspond to the word lines, and the elements of the inner lists correspond to the tab-separated fields. Take a moment to map these elements to the fields as they are described on the [CoNLL-U Format](http://universaldependencies.org/format.html) page. In particular, try to identify the list indices that correspond to the fields holding the word form, part-of-speech tag, and syntactic head of the relevant word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 1</div>\n",
    "<div class=\"panel-body\">\n",
    "Implement a generator function `trees()` that reads dependency trees stored in the CoNLL-U format and for each tree yields a triple consisting of the list of words in the sentence (strings), the corresponding list of part-of-speech tags (strings), and the syntactic heads for all of the words (integers).\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "This is how the output of the function `trees()` should look like for the sample sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['<ROOT>', 'From', 'the', 'AP', 'comes', 'this', 'story', ':'], ['<ROOT>', 'ADP', 'DET', 'PROPN', 'VERB', 'DET', 'NOUN', 'PUNCT'], [0, 3, 3, 4, 0, 6, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/TDDE09/labs/nlp4/en-ud-dev.conllu\") as fp:\n",
    "    print(next(nlp4.trees(fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you are supposed to convert the head fields from strings to integers. You are also supposed to pre-pad each sentence with a special pseudo-word `<ROOT>`. The part-of-speech tag of this pseudo-word should be `<ROOT>`, and its head should have index&nbsp;0.\n",
    "\n",
    "To solve Problem&nbsp;1, you should modify the following code cell. Please note that you are supposed to write this function as a [generator function](https://wiki.python.org/moin/Generators). In particular, the function should *not* return a list of triples. You may use the function `nlp4.conllu()` for reading the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trees(fp):\n",
    "    \"\"\"Reads trees from an input source.\n",
    "    \n",
    "    Args:\n",
    "        fp: A file pointer.\n",
    "    Yields:\n",
    "        Triples of the form words, tags, heads where: words is the list of\n",
    "        words of the tree (including the pseudo-word <ROOT> at position 0),\n",
    "        tags is the list of corresponding tags, and heads is the list of\n",
    "        head indices (one head index per word in the tree).\n",
    "    \"\"\"\n",
    "    tree =  nlp4.conllu(fp);\n",
    "    \n",
    "    for tree in nlp4.conllu(fp):\n",
    "        pos = list();\n",
    "        word = list();\n",
    "        Head = list();\n",
    "        bigList = list();\n",
    "        \n",
    "        pos.append('<ROOT>');\n",
    "        word.append('<ROOT>');\n",
    "        Head.append(0);\n",
    "        \n",
    "        for tokens in tree:\n",
    "            pos.append(tokens[1])\n",
    "            word.append(tokens[3])\n",
    "            Head.append(int(float(tokens[6])))\n",
    "            \n",
    "        bigList.append(pos);\n",
    "        bigList.append(word);\n",
    "        bigList.append(Head);\n",
    "    \n",
    "        yield bigList;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test you code by executing the following code cell. When you are done with Problem&nbsp;1, this should produce exactly the same output as the call to `nlp4.trees()` above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<ROOT>', 'From', 'the', 'AP', 'comes', 'this', 'story', ':'], ['<ROOT>', 'ADP', 'DET', 'PROPN', 'VERB', 'DET', 'NOUN', 'PUNCT'], [0, 3, 3, 4, 0, 6, 4, 4]]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/TDDE09/labs/nlp4/en-ud-dev.conllu\") as fp:\n",
    "    print(next(trees(fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The multi-class perceptron classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first core component of the baseline system is a multi-class perceptron classifier, very much like the one that you have implemented in lab&nbsp;1 (on text classification), and then again in lab&nbsp;3 (on part-of-speech tagging).\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Perceptron(nlp4.Perceptron):\n",
    "    \"\"\"A multi-class perceptron classifier.\n",
    "    \n",
    "    A multi-class perceptron consists of a number of cells, one cell for each\n",
    "    class. When a cell is presented with an input, it gets activated, and the\n",
    "    cell with the highest activation predicts the class for the input. An input\n",
    "    is represented by a feature vector, and the activation is computed by\n",
    "    taking the dot product (weighted sum) of this feature vector and the\n",
    "    cell-specific weight vector.\n",
    "    \n",
    "    This implementation of a multi-class perceptron assumes that both classes\n",
    "    and features can be used as dictionary keys. Feature vectors are\n",
    "    represented as lists of features.\n",
    "    \n",
    "    Attributes:\n",
    "        classes: A set containing the classes of this classifier.\n",
    "        weights: A nested dictionary mapping feature–class combinations to\n",
    "            class-specific feature weights (floats).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises a new classifier.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def predict(self, x, candidates=None):\n",
    "        \"\"\"Predicts the class for a feature vector.\n",
    "        \n",
    "        This computes the activations for the classes of this perceptron for\n",
    "        the feature vector `x` and returns the class with the highest\n",
    "        activation.\n",
    "        \n",
    "        Args:\n",
    "            x: A feature vector.\n",
    "            candidates: A list of candidate classes, or None if all classes\n",
    "                should be considered as candidates.\n",
    "        \n",
    "        Returns:\n",
    "            The candidate class with the highest activation. If two classes\n",
    "            have the same activation, this returns the class that is larger\n",
    "            with respect to the natural ordering on classes.\n",
    "        \"\"\"\n",
    "        return super().predict(x, candidates)\n",
    "\n",
    "    def update(self, x, y):\n",
    "        \"\"\"Updates the weight vectors with a single training example.\n",
    "        \n",
    "        Args:\n",
    "            x: A feature vector.\n",
    "            y: The gold-standard class for the specified feature vector.\n",
    "        \n",
    "        Returns:\n",
    "            The predicted class for the specified feature vector.\n",
    "        \"\"\"\n",
    "        return super().update(x, y)\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"Averages the weight vectors.\"\"\"\n",
    "        super().finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "#### Restrict predictions to candidate classes\n",
    "\n",
    "The only novelty of this implementation compared to the ones that you have produced earlier is in the `predict()` method. This method now takes an optional argument `candidates`, which may be either `None` or a list of classes. When it is the latter, the perceptron restricts its prediction to the specified classes, that is, it predicts the highest-scoring class from the list, rather than the highest-scoring class overall. This functionality will be useful in the parser.\n",
    "\n",
    "#### Tie-breaking\n",
    "\n",
    "Another detail in the implementation is that `predict()` breaks ties in the scoring by choosing the class that is larger with respect to the natural order on classes. For example, if classes are represented as strings then the classifier will choose the class that is largest with respect to the lexicographic order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The part-of-speech tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second core component of the baseline system is a part-of-speech tagger very similar to the one that you have implemented in lab&nbsp;3.\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tagger(nlp4.Tagger):\n",
    "    \"\"\"A part-of-speech tagger based on a multi-class perceptron classifier.\n",
    "    \n",
    "    This tagger implements a simple, left-to-right tagging algorithm where the\n",
    "    prediction of the tag for the next word in the sentence can be based on the\n",
    "    surrounding words and the previously predicted tags. The exact features\n",
    "    that this prediction is based on can be controlled with the `features()`\n",
    "    method, which should return a feature vector that can be used as an input\n",
    "    to the multi-class perceptron.\n",
    "    \n",
    "    Attributes:\n",
    "        classifier: A multi-class perceptron classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises a new tagger.\"\"\"\n",
    "        super().__init__()\n",
    "    \n",
    "    def tag(self, words):\n",
    "        \"\"\"Tags a sentence with part-of-speech tags.\n",
    "        \n",
    "        Args:\n",
    "            words: The input sentence, a list of words.\n",
    "        \n",
    "        Returns:\n",
    "            The list of predicted tags for the input sentence.\n",
    "        \"\"\"\n",
    "        return super().tag(words)\n",
    "    \n",
    "    def update(self, words, gold_tags):\n",
    "        \"\"\"Updates the tagger with a single training example.\n",
    "        \n",
    "        Args:\n",
    "            words: The list of words in the input sentence.\n",
    "            gold_tags: The list of gold-standard tags for the input sentence.\n",
    "        \n",
    "        Returns:\n",
    "            The list of predicted tags for the input sentence.\n",
    "        \"\"\"\n",
    "        return super().update(words, gold_tags)\n",
    "    \n",
    "    def features(self, words, i, pred_tags):\n",
    "        \"\"\"Extracts features for the specified tagger configuration.\n",
    "        \n",
    "        Args:\n",
    "            words: The input sentence, a list of words.\n",
    "            i: The index of the word that is currently being tagged.\n",
    "            pred_tags: The list of previously predicted tags.\n",
    "        \n",
    "        Returns:\n",
    "            A feature vector for the specified configuration.\n",
    "        \"\"\"\n",
    "        return super().features(words, i, pred_tags)\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"Averages the weight vectors.\"\"\"\n",
    "        super().finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "The reference implementation extracts the following features: current word, previous word, next word, and previous tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your main task in this lab is to implement the third and final core component of the baseline system, the parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 2</div>\n",
    "<div class=\"panel-body\">\n",
    "Implement a transition-based dependency parser, train it on the specified training data, and evaluate its performance on the specified development data. Your parser should get the same results as the reference implementation.\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Your starting point for this problem is the following skeleton class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Parser(nlp4.Parser):\n",
    "    \"\"\"A transition-based dependency parser.\n",
    "    \n",
    "    This parser implements the arc-standard algorithm for dependency parsing.\n",
    "    When being presented with an input sentence, it first tags the sentence for\n",
    "    parts of speech, and then uses a multi-class perceptron classifier to\n",
    "    predict a sequence of *moves* (transitions) that construct a dependency\n",
    "    tree for the input sentence. Moves are encoded as integers as follows:\n",
    "    \n",
    "    SHIFT = 0, LEFT-ARC = 1, RIGHT-ARC = 2\n",
    "    \n",
    "    At any given point in the predicted sequence, the state of the parser can\n",
    "    be specified by: the index of the first word in the input sentence that\n",
    "    the parser has not yet started to process; a stack holding the indices of\n",
    "    those words that are currently being processed; and a partial dependency\n",
    "    tree, represented as a list of indices such that `tree[i]` gives the index\n",
    "    of the head (parent node) of the word at position `i`, or 0 in case the\n",
    "    corresponding word has not yet been assigned a head.\n",
    "    \n",
    "    Attributes:\n",
    "        tagger: A part-of-speech tagger.\n",
    "        classifier: A multi-class perceptron classifier used to predict the\n",
    "            next move of the parser.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises a new parser.\"\"\"\n",
    "        self.tagger = Tagger();\n",
    "        self.classifier = Perceptron();\n",
    "        #super().__init__()\n",
    "    \n",
    "    def parse(self, words):\n",
    "        \"\"\"Parses a sentence.\n",
    "        \n",
    "        Args:\n",
    "            words: The input sentence, a list of words.\n",
    "        \n",
    "        Returns:\n",
    "            A pair consisting of the predicted tags and the predicted\n",
    "            dependency tree for the input sentence.\n",
    "        \n",
    "        \"\"\"\n",
    "        pred_tree = []\n",
    "        \n",
    "        for j in range(0,len(words)):\n",
    "            pred_tree.append(0) #Börjar med nollor i trädet\n",
    "        \n",
    "        pred_tags = self.tagger.tag(words) #Predicerade taggar\n",
    "        stack = [] #Stacken initialiseras som tom\n",
    "        i = 0\n",
    "        while self.valid_moves(i,stack,pred_tree) != []:\n",
    "                x = self.features(words,pred_tags,i,stack,pred_tree)#Features\n",
    "                candidates = self.valid_moves(i,stack,pred_tree) #Valid moves\n",
    "                move = self.classifier.predict(x,candidates) #Predict the dependency\n",
    "\n",
    "                temp_list = self.move(i,stack,pred_tree,move) #List med bl.a. stack, pred_tree\n",
    "                stack = temp_list[1] #Extraherar stacken\n",
    "                pred_tree = temp_list[2] #Extraherar predicerade trädet\n",
    "                i = temp_list[0] #Extraherar indexet\n",
    "            \n",
    "        return (pred_tags,pred_tree)\n",
    "        \n",
    "        #return super().parse(words)\n",
    "    \n",
    "    def valid_moves(self, i, stack, pred_tree):\n",
    "        \"\"\"Returns the valid moves for the specified parser configuration.\n",
    "        \n",
    "        Args:\n",
    "            i: The index of the first unprocessed word.\n",
    "            stack: The stack of words that are currently being processed.\n",
    "            pred_tree: The partial dependency tree.\n",
    "        \n",
    "        Returns:\n",
    "            The list of valid moves for the specified parser configuration.\n",
    "        \n",
    "        \"\"\"\n",
    "        valid_moves = list()\n",
    "        \n",
    "        if(i < len(pred_tree)):\n",
    "            valid_moves.append(0)\n",
    "            \n",
    "        if(len(stack) >= 2):\n",
    "            valid_moves.append(2)\n",
    "            if(len(stack)>=3):\n",
    "                valid_moves.append(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return valid_moves\n",
    "        \n",
    "        #return super().valid_moves(i, stack, pred_tree)\n",
    "    \n",
    "    def move(self, i, stack, pred_tree, move):\n",
    "        \"\"\"Executes a single move.\n",
    "        \n",
    "        Args:\n",
    "            i: The index of the first unprocessed word.\n",
    "            stack: The stack of words that are currently being processed.\n",
    "            pred_tree: The partial dependency tree.\n",
    "            move: The move that the parser should make.\n",
    "        \n",
    "        Returns:\n",
    "            The new parser configuration, represented as a triple containing\n",
    "            the index of the new first unprocessed word, stack, and partial\n",
    "            dependency tree.\n",
    "        \n",
    "        \"\"\"\n",
    "        if(move == 0):\n",
    "            stack.append(i);\n",
    "            i = i+1;\n",
    "        if(move == 1):\n",
    "            headIndex = stack.pop();\n",
    "            tempIndex = stack.pop();\n",
    "            pred_tree[tempIndex] = headIndex;\n",
    "            stack.append(headIndex);\n",
    "        if(move == 2):\n",
    "            tempIndex = stack.pop();\n",
    "            headIndex = stack.pop();\n",
    "            pred_tree[tempIndex] = headIndex;\n",
    "            stack.append(headIndex);\n",
    "        \n",
    "        return (i,stack,pred_tree);\n",
    "        \n",
    "\n",
    "        #return super().move(i, stack, pred_tree, move)\n",
    "    \n",
    "    def update(self, words, gold_tags, gold_tree):\n",
    "        \"\"\"Updates the move classifier with a single training example.\n",
    "        \n",
    "        Args:\n",
    "            words: The input sentence, a list of words.\n",
    "            gold_tags: The list of gold-standard tags for the input sentence.\n",
    "            gold_tree: The gold-standard tree for the sentence.\n",
    "        \n",
    "        Returns:\n",
    "            A pair consisting of the predicted tags and the predicted\n",
    "            dependency tree for the input sentence.\n",
    "        \"\"\"\n",
    "        \n",
    "        pred_tree = []\n",
    "        \n",
    "        for j in range(0,len(words)):\n",
    "            pred_tree.append(0) #Börjar med nollor i trädet\n",
    "            \n",
    "        self.tagger.update(words,gold_tags)\n",
    "        pred_tags = self.tagger.tag(words)\n",
    "        stack = [] #Stacken initialiseras som tom\n",
    "        i = 0\n",
    "        while self.valid_moves(i,stack,pred_tree) != []:\n",
    "                x = self.features(words,pred_tags,i,stack,pred_tree)#Features\n",
    "                gold_move = self.gold_move(i,stack,pred_tree,gold_tree)\n",
    "                self.classifier.update(x,gold_move)             \n",
    "\n",
    "                temp_list = self.move(i,stack,pred_tree,gold_move) #List med index, stack, pred_tree\n",
    "                i = temp_list[0] #Extraherar indexet\n",
    "                stack = temp_list[1] #Extraherar stacken\n",
    "                pred_tree = temp_list[2] #Extraherar predicerade trädet\n",
    "        \n",
    "        return (pred_tags,pred_tree)\n",
    "    \n",
    "    def gold_move(self, i, stack, pred_tree, gold_tree):\n",
    "        \"\"\"Returns the gold-standard move for the specified parser\n",
    "        configuration.\n",
    "        \n",
    "        The gold-standard move is the first possible move from the following\n",
    "        list: LEFT-ARC, RIGHT-ARC, SHIFT. LEFT-ARC is possible if the topmost\n",
    "        word on the stack is the gold-standard head of the second-topmost word,\n",
    "        and all words that have the second-topmost word on the stack as their\n",
    "        gold-standard head have already been assigned their head in the\n",
    "        predicted tree. Symmetric conditions apply to RIGHT-ARC. SHIFT is\n",
    "        possible if at least one word in the input sentence still requires\n",
    "        processing.\n",
    "        \n",
    "        Args:\n",
    "            i: The index of the first unprocessed word.\n",
    "            stack: The stack of words that are currently being processed.\n",
    "            pred_tree: The partial dependency tree.\n",
    "            gold_tree: The gold-standard dependency tree.\n",
    "        \n",
    "        Returns:\n",
    "            The gold-standard move for the specified parser configuration, or\n",
    "            None if no move is possible.\n",
    "       \n",
    "        \"\"\"\n",
    "        left = False;\n",
    "        right = False;\n",
    "        \n",
    "        validmoves = self.valid_moves(i,stack,pred_tree)\n",
    "        \n",
    "        if (1 in validmoves and stack[-1] == gold_tree[stack[-2]]):\n",
    "            left = True;\n",
    "            for i in range(0,len(gold_tree)):\n",
    "                if(gold_tree[i] == stack[-2] and pred_tree[i] == 0):\n",
    "                    left = False;\n",
    "                    \n",
    "        if (2 in validmoves and stack[-2] == gold_tree[stack[-1]]):\n",
    "            right = True;\n",
    "            for i in range(0,len(gold_tree)):\n",
    "                if(gold_tree[i] == stack[-1] and pred_tree[i] == 0):\n",
    "                    right = False;\n",
    "    \n",
    "        if(left):\n",
    "            return 1;\n",
    "        if(right):\n",
    "            return 2;\n",
    "        else:\n",
    "            return 0;\n",
    "        #return super().gold_move(i, stack, pred_tree, gold_tree)\n",
    "    \n",
    "    def features(self, words, tags, i, stack, parse):\n",
    "        \"\"\"Extracts features for the specified parser configuration.\n",
    "        \n",
    "        Args:\n",
    "            i: The index of the first unprocessed word.\n",
    "            stack: The stack of words that are currently being processed.\n",
    "            pred_tree: The partial dependency tree.\n",
    "        \n",
    "        Returns:\n",
    "            A feature vector for the specified configuration.\n",
    "        \"\"\"\n",
    "        features = list();\n",
    "        \n",
    "        if(i >= len(words)):\n",
    "            features.append((0,\"<EOS>\"))\n",
    "            features.append((1,\"<EOS>\"))\n",
    "        else:\n",
    "            features.append((0,words[i]))\n",
    "            features.append((1,tags[i]))\n",
    "            \n",
    "        if(len(stack) > 0):\n",
    "            features.append((2,words[stack[-1]]));\n",
    "            features.append((3,tags[stack[-1]]));\n",
    "        else:\n",
    "            features.append((2,\"<Empty>\"));\n",
    "            features.append((3,\"<Empty>\"));\n",
    "            \n",
    "        if(len(stack) > 1):\n",
    "            features.append((4,words[stack[-2]]));\n",
    "            features.append((5,tags[stack[-2]]));\n",
    "        else:\n",
    "            features.append((4,\"<Empty>\"));\n",
    "            features.append((5,\"<Empty>\"));\n",
    "            \n",
    "        return features;\n",
    "        #return super().features(words, tags, i, stack, parse)\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"Averages the weight vectors.\"\"\"\n",
    "        super().finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "#### Main parsing method\n",
    "\n",
    "The first step in the parsing is to tag the sentence for parts of speech. After tagging, the `parse()` method initialises a new parser configuration and asks the move classifier to predict the move that the parser should make in that configuration. It is important that this prediction is restricted to predict a valid move; this is where the optional argument of the `predict()` method in the multi-class perceptron becomes relevant (see above). Making the predicted move results in a new configuration, for which the process is iterated; this continues as long as there are valid moves left to make.\n",
    "\n",
    "#### Extract features\n",
    "\n",
    "The reference implementation extracts the following features:\n",
    "\n",
    "* word form of the next word to be processed\n",
    "* tag of the next word to be processed\n",
    "* word form of the topmost word on the stack\n",
    "* tag of the topmost word on the stack\n",
    "* word form of the second-topmost word on the stack\n",
    "* tag of the second-topmost word on the stack\n",
    "\n",
    "In your own implementation, make sure to handle the special cases where there are no more words left that need to be processed, or the stack is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to train and evaluate your parser. You can control the number of examples used for training by setting the value `n_examples`. The second cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated with sentence #2000\n",
      "Parsing sentence #2001\n",
      "Tagging accuracy: 78.34%\n",
      "Unlabelled attachment score: 55.41%\n"
     ]
    }
   ],
   "source": [
    "n_examples = 2000   # Set to None to train on all examples\n",
    "\n",
    "parser = Parser()\n",
    "with open(\"/home/TDDE09/labs/nlp4/en-ud-train-projective.conllu\") as fp:\n",
    "    for i, (words, gold_tags, gold_tree) in enumerate(trees(fp)):\n",
    "        parser.update(words, gold_tags, gold_tree)\n",
    "        print(\"\\rUpdated with sentence #{}\".format(i), end=\"\")\n",
    "        if n_examples and i >= n_examples:\n",
    "            break\n",
    "    print(\"\")\n",
    "parser.finalize()\n",
    "\n",
    "acc_k = acc_n = 0\n",
    "uas_k = uas_n = 0\n",
    "with open(\"/home/TDDE09/labs/nlp4/en-ud-dev.conllu\") as fp:\n",
    "    for i, (words, gold_tags, gold_tree) in enumerate(trees(fp)):\n",
    "        pred_tags, pred_tree = parser.parse(words)\n",
    "        acc_k += sum(int(g == p) for g, p in zip(gold_tags, pred_tags)) - 1\n",
    "        acc_n += len(words) - 1\n",
    "        uas_k += sum(int(g == p) for g, p in zip(gold_tree, pred_tree)) - 1\n",
    "        uas_n += len(words) - 1\n",
    "        print(\"\\rParsing sentence #{}\".format(i), end=\"\")\n",
    "    print(\"\")\n",
    "print(\"Tagging accuracy: {:.2%}\".format(acc_k / acc_n))\n",
    "print(\"Unlabelled attachment score: {:.2%}\".format(uas_k / uas_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your own implementation you should obtain scores very similar to the ones of the reference implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.34"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "78.34 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.41"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "55.41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
